{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6iFEplQQk_G"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Activation\n",
        "from tensorflow.keras.optimizers.experimental import RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Através dessa função o TensorFlow é capaz de baixar o texto diretamente do link\n",
        "#filepath = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "filepath = tf.keras.utils.get_file('bible.txt','https://www.gutenberg.org/files/62383/62383-0.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWuNnki-Qsg1",
        "outputId": "392abff1-eec4-4a59-f694-2b7812673811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.gutenberg.org/files/62383/62383-0.txt\n",
            "5162002/5162002 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tornar as palavras minuscuras e mudar o encoding para utf-8, ajuda na acuária do modelo\n",
        "text = open(filepath, 'rb').read().decode(encoding='utf-8').lower()"
      ],
      "metadata": {
        "id": "YFncxUNFQ_Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text.title()"
      ],
      "metadata": {
        "id": "quHV32c6Rjw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separar o conteúdo dos dataset\n",
        "Iremos utilizar somente uma parte dos textos para treinar.\n",
        "\n",
        "Quanto maior os dados de entrada mais preciso e fidedigno o modelo.\n",
        "Entretanto maior o tempo e consumo computacional.\n",
        "\n",
        "O  problema do consumo computacional é diminuido devido o colab, entretnto o tempo consitnua sendo um problema."
      ],
      "metadata": {
        "id": "5pFmWbWqUBDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = text[300000:800000]"
      ],
      "metadata": {
        "id": "j7beQ2qQSv7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atribuir valores aos caracteres\n",
        "\n",
        "Os modelos de ML apenas entendem números como entrada. Então uma das alternativas ao se trabalhar com NLP é dar valores númericos aos characteres.\n",
        "\n",
        "Essa etapa fica ainda mais fácil ao normalizamos o texto em minisculas e na utf-8."
      ],
      "metadata": {
        "id": "SGyeOsuYUVn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(set(text))\n",
        "#print(characters)"
      ],
      "metadata": {
        "id": "FsTDvTI8S-XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index=dict((c,i) for i,c in enumerate(characters))\n",
        "#print(char_to_index)"
      ],
      "metadata": {
        "id": "QivrcPq2Tbx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char=dict((i,c) for i,c in enumerate(characters))\n",
        "#print(index_to_char)"
      ],
      "metadata": {
        "id": "qag-SBCGU8iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Previsão de frases\n",
        "\n",
        "Consiste em prever o proximo caracters através dos carcteres anteriores. Lembrou-me Bayes....\n",
        "\n"
      ],
      "metadata": {
        "id": "Lp6Z0Bx7Vg_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Parametos necessários\n",
        "\n",
        "# Tamanhos máximo das frases\n",
        "SEQ_LENGTH= 40\n",
        "\n",
        "#Tamanhos do conjuntos de caracteres a serem analisados por vez\n",
        "STEP_SIZE = 3\n",
        "\n",
        "sentences=[]\n",
        "next_characters=[]"
      ],
      "metadata": {
        "id": "J5ZrnLBrVpTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
        "  sentences.append(text[i : i+SEQ_LENGTH])\n",
        "  next_characters.append(text[i+SEQ_LENGTH])"
      ],
      "metadata": {
        "id": "lQDIKFZLW8nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tranformando os dados gerados em um vetor númerico"
      ],
      "metadata": {
        "id": "UUVUt1_iYNOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forma uma matriz de zeros informando se tal caractere está na frase ou não, con true ou false.\n",
        "# Essas matrizes serão nossas variares preditoras e resposta\n",
        "x = np.zeros((len(sentences),SEQ_LENGTH, len(characters)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(characters)), dtype=np.bool)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lit3SPSEX6-E",
        "outputId": "e43dda87-0efb-4674-f8aa-c7c263fbf137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-70-ad36a7901484>:3: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x = np.zeros((len(sentences),SEQ_LENGTH, len(characters)), dtype=np.bool)\n",
            "<ipython-input-70-ad36a7901484>:4: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y = np.zeros((len(sentences), len(characters)), dtype=np.bool)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(x)\n",
        "#print(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-Wh5sTHaMzk",
        "outputId": "20fb3f2c-a543-453a-df38-3e0a0793b132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[False  True False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]\n",
            "\n",
            " [[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]\n",
            "\n",
            " [[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]\n",
            "\n",
            " [[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]\n",
            "\n",
            " [[False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  ...\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]\n",
            "  [False False False ... False False False]]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Substituindo os valores da martriz pelos caracterstes correspondentes\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, character in enumerate(sentence):\n",
        "    x[i,t, char_to_index[character]] = 1\n",
        "  y[i, char_to_index[next_characters[i]]] =1\n"
      ],
      "metadata": {
        "id": "QY9lBP_Ja-g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criando as Redes Neurais"
      ],
      "metadata": {
        "id": "KbbQubX4fcjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(120,input_shape=(SEQ_LENGTH,len(characters))))\n",
        "model.add(Dense(len(characters)))\n",
        "model.add(Activation('softmax'))\n",
        "\n"
      ],
      "metadata": {
        "id": "oUNvil6FfMFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.engine.training import optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(learning_rate=0.01))\n",
        "model.fit(x,y, batch_size=254, epochs=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT55X_sihRBu",
        "outputId": "0db8fd8d-f0ad-4210-b2d5-017637ed24f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "657/657 [==============================] - 72s 108ms/step - loss: 1.9364\n",
            "Epoch 2/4\n",
            "657/657 [==============================] - 70s 107ms/step - loss: 1.4290\n",
            "Epoch 3/4\n",
            "657/657 [==============================] - 71s 109ms/step - loss: 1.2899\n",
            "Epoch 4/4\n",
            "657/657 [==============================] - 70s 107ms/step - loss: 1.2175\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1cfb9d7be0>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('textgenerator.model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXl41du-iMwT",
        "outputId": "1550d8e9-0594-436e-f238-ffa26b762ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2=tf.keras.models.load_model('textgenerator.model')"
      ],
      "metadata": {
        "id": "7rJ00pbOmi6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions, fornecida pela propria equipe do Keras.\n",
        "#Ela seleciona a melhor probabilidade\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "metadata": {
        "id": "ftXmyuyAnSCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Função que irá gerar o texto"
      ],
      "metadata": {
        "id": "VVUzmWUGoSpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(length, temperature):\n",
        "  start_index = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "  generated =''\n",
        "  sentence = text[start_index: start_index + SEQ_LENGTH]\n",
        "  generated += sentence\n",
        "  for i in range(length):\n",
        "    x = np.zeros((1, SEQ_LENGTH, len(characters)))\n",
        "    for t,character in enumerate(sentence):\n",
        "      x[0,t,char_to_index[character]] = 1\n",
        "\n",
        "    predictions = model.predict(x,verbose=0)[0]\n",
        "    next_index= sample(predictions,temperature)\n",
        "\n",
        "    next_character = index_to_char[next_index]\n",
        "\n",
        "    generated += next_character\n",
        "    sentence = sentence[1:] + next_character\n",
        "\n",
        "  return generated\n"
      ],
      "metadata": {
        "id": "tNqHmwFooYjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('------- Temperature: 0.2 ------------')\n",
        "print(generate_text(300,0.2))\n",
        "print('------- Temperature: 0.4 --------------- ')\n",
        "print(generate_text(300,0.4))\n",
        "print(' --------------- Temperature: 0.8 ---------------')\n",
        "print(generate_text(300,0.8))\n",
        "print('--------------- Temperature: 1 ---------------')\n",
        "print(generate_text(300,1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIcYE66Uqys-",
        "outputId": "4263da29-b228-42cd-bcf9-6167f9c6cc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- Temperature: 0.2 ------------\n",
            "us: estatuto perpetuo\n",
            "_é_ por vossas gerações, e o sacerdote o sacerdote o sacerdote o sacerdote o sacerdote o sacerdote o sacerdote o sacerdote o senhor.\n",
            "\n",
            "22 e a terra do sacerdote o sacerdote o sacerdote o senhor.\n",
            "\n",
            "22 e o sacerdote o sacerdote o sacerdote o senhor.\n",
            "\n",
            "22 e a sua mão será sacerdote, e a sua mão será sacerdote, e o s\n",
            "------- Temperature: 0.4 --------------- \n",
            "s _que foram_ contados d’elles, _foram_ a algum alter de um carneiro.\n",
            "\n",
            "2 e a sua mão sanctos de o sacerdote o sacerdote o sacerdote, [23] a sua mão de moysés, assim o sacerdote o senhor.\n",
            "\n",
            "2 esta famina de deus povo.\n",
            "\n",
            "31 e não filho de deus animas, que o sangue do senhor.\n",
            "\n",
            "23 e disse a sua carne até não os carneiros de deus.\n",
            "\n",
            "22 \n",
            " --------------- Temperature: 0.8 ---------------\n",
            "ue lhes não tomassemos: sessenta cidades;\n",
            "\n",
            "23 e o sabba _para_ altados da expiação.\n",
            "\n",
            "23 fallada mão da mão será omará oquente, na bura incentos, e amaldiça;\n",
            "\n",
            "22 gerarás a porão a tribu de dazer.\n",
            "\n",
            "26 astita que _estava_ ammundira, [5] e da tabelio, terias.\n",
            "\n",
            "3 e será sobiresto samber da fostua o servo, e o dengro,\n",
            "\n",
            "4 para outro \n",
            "--------------- Temperature: 1 ---------------\n",
            "3.\n",
            "\n",
            "[18] agg. 2.13. lev. 15.5.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "_e a sua mão [2] dias tambem chana praba, de riundirimo, ourivo, [22] de astithor, quando cortiram [7] cheirias: e ] obrigo.\n",
            "\n",
            "2 filando lhes, que não farás, de contincad,\n",
            "em qualeas para lhes.\n",
            "\n",
            "33 nenhuma.\n",
            "\n",
            "14 o sabba _é_ a sua parabon.\n",
            "\n",
            "24 expia, faverá [3_] pela boa de\n",
            "semictos, ou\n",
            "in\n"
          ]
        }
      ]
    }
  ]
}